{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align:center;\">Data 3402: Project 1\n",
    "## <p style=\"text-align:center;\">Gathering Raw Data\n",
    "### <p style=\"text-align:center;\">David Wiley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Data 3401, where projects were disjointed and unrelated, in this course, we will work\n",
    "a series of projects that build on one another, so that by the end of the semester, you will\n",
    "have built an engine that will extract raw data from multiple sources (freely available on the\n",
    "internet), process (parse, clean, transform, merge, etc) that raw data, and store it in a database\n",
    "for future analysis. In this first project, you will use bash scripting to download, organize,\n",
    "and store raw data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center;\">The Data and Your Task\n",
    "\n",
    "Baseball, more than any other sport, has become a business that is driven by data. There are\n",
    "vast quantities of data freely available through web, so much, in fact, that entire games can\n",
    "be constructed down to the path, velocity, and rotation of each pitch. While we will not be\n",
    "dealing with the data to this minute level, we will be gathering data from three major sources.\n",
    "\n",
    "1. MLB.com\n",
    "2. Retrosheets.org\n",
    "3. The Lahman Database\n",
    "\n",
    "Your task will be to write a multithreaded python script that gathers data from each of these\n",
    "sources (specified in the following sections) and stores the data according to the scheme that\n",
    "will be laid out.\n",
    "Your script should take two arguments:\n",
    "\n",
    "1. a number of threads, and\n",
    "2. a directory to store the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MLB.com\n",
    "\n",
    "We will be particularly interested in the MLB gameday data that is found at https://gd2.mlb.com/components/game/mlb/ . You can navigate the links, but what you need to know is that game data for a specific date MM/DD/YYYY can be found at:\n",
    "\n",
    "https://gd2.mlb.com/components/game/mlb/year_YYYY/month_MM/day_DD\n",
    "\n",
    "Notice that I do not include a ’/’ at the end of that URL. Doing so will result in an error.\n",
    "On this page, you will see a number of links with names of the form **gid_YYYY_MM_DD_***.\n",
    "\n",
    "Each of these is a directory that stores data for the specified game. For example,\n",
    "**gid_2018_07_01_anamlb_balmlb_1/** contains data for the game played between the Anaheim Angles and the Baltimore Orioles on 07/01/2018. The ’1’ at the end indicates that this was the first game played between the two on that date. A ’2’ would indicate that the game was the second game in a double-header. The names of these directories are the ”Game IDs” (GID).\n",
    "\n",
    "Inside this directory, there are three files that you should capture.\n",
    "\n",
    "1. **players.xml**: contains information\n",
    "2. **inning all.xml**: contains the result of every event (pitch, pickoff attempt, steal attempt, etc.) that occurred during the game.\n",
    "3. **inning hit.xml**: contains information about every hit (including location where the ball landed or first touched a fielder).\n",
    "\n",
    "Your script should do the following:\n",
    "\n",
    "1. Create a directory called MLB\n",
    "2. Inside **MLB**, create a directory for each game played between 01/01/2010 and the current date. The name of each game directory should be its GID.\n",
    "3. In each game directory, you store the three files **players.xml, inning all.xml**, and **inning hit.xml**.\n",
    "\n",
    "These tasks should be accomplished using a pool of threads. The pool should:\n",
    "\n",
    "1. take a list of dates (or date objects)\n",
    "2. extract the GIDs for that date and add them to a queue\n",
    "3. read the games from the queue and download the game data files.\n",
    "\n",
    "I know this one was a bit complicated to explain, but don’t worry, the other two sources should be a bit straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting game IDs...\n",
      "\n",
      "2019-09-05 done.\n",
      "2019-09-06 done.\n",
      "2019-09-02 done.\n",
      "2019-09-19 done.\n",
      "2019-09-10 done.\n",
      "2019-09-09 done.\n",
      "2019-09-01 done.\n",
      "2019-09-16 done.\n",
      "2019-09-04 done.\n",
      "2019-09-13 done.\n",
      "2019-09-18 done.\n",
      "2019-09-11 done.\n",
      "2019-09-03 done.\n",
      "2019-09-07 done.\n",
      "2019-09-15 done.\n",
      "2019-09-17 done.\n",
      "2019-09-12 done.\n",
      "2019-09-14 done.\n",
      "2019-09-20 done.\n",
      "2019-09-21 done.\n",
      "2019-09-08 done.\n",
      "2019-09-27 done.\n",
      "2019-09-25 done.\n",
      "2019-09-26 done.\n",
      "2019-10-03 done.\n",
      "2019-10-09 done.\n",
      "2019-10-10 done.\n",
      "2019-09-22 done.\n",
      "2019-10-01 done.\n",
      "2019-09-23 done.\n",
      "2019-10-07 done.\n",
      "2019-09-24 done.\n",
      "2019-09-29 done.\n",
      "2019-10-04 done.\n",
      "2019-10-05 done.\n",
      "2019-10-06 done.\n",
      "2019-10-11 done.\n",
      "2019-09-28 done.\n",
      "2019-10-02 done.\n",
      "2019-10-08 done.\n",
      "2019-09-30 done.\n",
      "2019-10-21 done.\n",
      "2019-10-12 done.\n",
      "2019-10-13 done.\n",
      "2019-10-20 done.\n",
      "2019-10-15 done.\n",
      "2019-10-17 done.\n",
      "2019-10-19 done.\n",
      "2019-10-14 done.\n",
      "2019-10-18 done.\n",
      "2019-10-16 done.\n",
      "Finished getting game IDs.\n",
      "Time Elapsed: 1.6 seconds.\n",
      "Working on processing game IDs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-788:\n",
      "Process ForkPoolWorker-791:\n",
      "Process ForkPoolWorker-796:\n",
      "Process ForkPoolWorker-798:\n",
      "Process ForkPoolWorker-800:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 105, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 105, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"<ipython-input-51-d83af2b3b1be>\", line 46, in process_GIDs\n",
      "    wget.download(url+'/inning/inning_all.xml', file_path +'/inning_all.xml')\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 105, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"<ipython-input-51-d83af2b3b1be>\", line 42, in process_GIDs\n",
      "    wget.download(url+'/players.xml', file_path+'/players.xml')\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/site-packages/wget.py\", line 526, in download\n",
      "    (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 276, in urlretrieve\n",
      "    block = fp.read(bs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 105, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"<ipython-input-51-d83af2b3b1be>\", line 46, in process_GIDs\n",
      "    wget.download(url+'/inning/inning_all.xml', file_path +'/inning_all.xml')\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 105, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"<ipython-input-51-d83af2b3b1be>\", line 46, in process_GIDs\n",
      "    wget.download(url+'/inning/inning_all.xml', file_path +'/inning_all.xml')\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/site-packages/wget.py\", line 526, in download\n",
      "    (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 447, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/site-packages/wget.py\", line 526, in download\n",
      "    (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 491, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 247, in urlretrieve\n",
      "    with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 276, in urlretrieve\n",
      "    block = fp.read(bs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/site-packages/wget.py\", line 526, in download\n",
      "    (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n",
      "  File \"<ipython-input-51-d83af2b3b1be>\", line 46, in process_GIDs\n",
      "    wget.download(url+'/inning/inning_all.xml', file_path +'/inning_all.xml')\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 447, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 276, in urlretrieve\n",
      "    block = fp.read(bs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 447, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 491, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 491, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/site-packages/wget.py\", line 526, in download\n",
      "    (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 276, in urlretrieve\n",
      "    block = fp.read(bs)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 543, in _open\n",
      "    '_open', req)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 447, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 503, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 491, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 1360, in https_open\n",
      "    context=self._context, check_hostname=self._check_hostname)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/urllib/request.py\", line 1320, in do_open\n",
      "    r = h.getresponse()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 1321, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 296, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/http/client.py\", line 257, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/src/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "from multiprocessing import Pool, Queue, Process\n",
    "from requests.exceptions import HTTPError\n",
    "import requests as rq, os, re, time, wget, sys, queue\n",
    "\n",
    "def get_GIDs(date):\n",
    "\n",
    "        try:\n",
    "\n",
    "            url = 'https://gd2.mlb.com/components/game/mlb/year_'+str(date.year)+'/month_'+str(date.month).zfill(2)+'/day_'+str(date.day).zfill(2)\n",
    "            r = rq.get(url)#, timeout=3.05, allow_redirects = True)\n",
    "            date_string = r.text\n",
    "\n",
    "            gid_rx = \"\\s{1}\\w+_[0-9]{4}_[0-9]{2}_[0-9]{2}_\\w+_\\w+_[0-9]\"\n",
    "\n",
    "            match = re.finditer(gid_rx, date_string)\n",
    "\n",
    "            for m in match:\n",
    "                q.put((m.group()).strip())\n",
    "            print(str(date)+ \" done.\")\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "def process_GIDs(q):\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            gid = q.get_nowait()\n",
    "            date_rx = '\\d{2,4}'\n",
    "            gid_date = re.findall(date_rx, gid)\n",
    "            file_path = './MLB/'+gid\n",
    "\n",
    "            url = 'https://gd2.mlb.com/components/game/mlb/year_'+str(gid_date[0])+'/month_'+str(gid_date[1])+'/day_'+str(gid_date[2])+'/'+gid\n",
    "\n",
    "            if not (os.path.exists('./MLB/')):\n",
    "                os.mkdir('./MLB/')\n",
    "            if not (os.path.exists('./MLB/'+ gid +'/')):\n",
    "                os.mkdir('./MLB/'+ gid +'/')\n",
    "            if not (os.path.exists('./MLB/'+ gid +'/players.xml')):\n",
    "                wget.download(url+'/players.xml', file_path+'/players.xml')\n",
    "            if not(os.path.exists('./MLB/'+ gid +'/inning/inning_hit.xml')):\n",
    "                wget.download(url+'/inning/inning_hit.xml', file_path +'/inning_hit.xml')\n",
    "            if not(os.path.exists('./MLB/'+ gid +'/inning/inning_all.xml')):\n",
    "                wget.download(url+'/inning/inning_all.xml', file_path +'/inning_all.xml')\n",
    "                \n",
    "        except queue.Empty:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if(__name__==\"__main__\"):\n",
    "    \n",
    "\n",
    "    \n",
    "#     num_threads = sys.argv[1]\n",
    "#     file_path = sys.argv[2]\n",
    "    start_time = time.time()\n",
    "        \n",
    "    date = date(2019, 9, 1)\n",
    "    today = date.today()\n",
    "     \n",
    "    q = Queue()\n",
    "    dates = []\n",
    "\n",
    "        \n",
    "    while(date <= today):\n",
    "        dates.append(date)\n",
    "        date += timedelta(1)\n",
    "        \n",
    "        \n",
    "    print(\"Getting game IDs...\\n\")\n",
    "    \n",
    "\n",
    "    gid_pool = Pool(20)\n",
    "    gid_pool.map(get_GIDs, dates)\n",
    "    gid_pool.close()\n",
    "    gid_pool.join()\n",
    "    \n",
    "    elapse_1 = time.time() - start_time\n",
    "    print(\"Finished getting game IDs.\")\n",
    "    print(\"Time Elapsed: {0} seconds.\".format(round(elapse_1, 2)))\n",
    "    print(\"Working on processing game IDs...\\n\")\n",
    "    \n",
    "    process_pool = Pool(30, process_GIDs, (q,))\n",
    "    process_pool.close()\n",
    "    process_pool.join()\n",
    "    elapse_2 = time.time() - start_time\n",
    "    print(\"Finished processing game IDs.\")\n",
    "    print(\"Time Elapsed: {0} seconds.\".format(round(elapse_2, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gid_2019_09_21_chamlb_detmlb_1\n",
      "gid_2019_09_21_colmlb_lanmlb_1\n",
      "gid_2019_09_21_kcamlb_minmlb_1\n",
      "gid_2019_09_21_nynmlb_cinmlb_1\n",
      "gid_2019_09_21_phimlb_clemlb_1\n",
      "gid_2019_09_21_pitmlb_milmlb_1\n",
      "gid_2019_09_21_seamlb_balmlb_1\n",
      "gid_2019_09_21_sfnmlb_atlmlb_1\n",
      "gid_2019_09_21_slnmlb_chnmlb_1\n",
      "gid_2019_09_21_texmlb_oakmlb_1\n",
      "gid_2019_09_21_tormlb_nyamlb_1\n",
      "gid_2019_09_21_wasmlb_miamlb_1\n",
      "gid_2019_09_20_anamlb_houmlb_1\n",
      "gid_2019_09_20_arimlb_sdnmlb_1\n",
      "gid_2019_09_20_bosmlb_tbamlb_1\n",
      "gid_2019_09_20_chamlb_detmlb_1\n",
      "gid_2019_09_20_colmlb_lanmlb_1\n",
      "gid_2019_09_20_kcamlb_minmlb_1\n",
      "gid_2019_09_20_nynmlb_cinmlb_1\n",
      "gid_2019_09_20_phimlb_clemlb_1\n",
      "gid_2019_09_20_pitmlb_milmlb_1\n",
      "gid_2019_09_20_seamlb_balmlb_1\n",
      "gid_2019_09_20_sfnmlb_atlmlb_1\n",
      "gid_2019_09_20_slnmlb_chnmlb_1\n",
      "gid_2019_09_20_texmlb_oakmlb_1\n",
      "gid_2019_09_20_tormlb_nyamlb_1\n",
      "gid_2019_09_20_wasmlb_miamlb_1\n",
      "gid_2019_09_23_balmlb_tormlb_1\n",
      "gid_2019_09_23_bosmlb_tbamlb_1\n",
      "gid_2019_09_23_miamlb_nynmlb_1\n",
      "gid_2019_09_23_phimlb_wasmlb_1\n",
      "gid_2019_09_23_slnmlb_arimlb_1\n",
      "gid_2019_09_09_arimlb_nynmlb_1\n",
      "gid_2019_09_09_atlmlb_phimlb_1\n",
      "gid_2019_09_09_chnmlb_sdnmlb_1\n",
      "gid_2019_09_09_clemlb_anamlb_1\n",
      "gid_2019_09_09_milmlb_miamlb_1\n",
      "gid_2019_09_09_nyamlb_bosmlb_1\n",
      "gid_2019_09_09_oakmlb_houmlb_1\n",
      "gid_2019_09_09_pitmlb_sfnmlb_1\n",
      "gid_2019_10_03_slnmlb_atlmlb_1\n",
      "gid_2019_10_03_wasmlb_lanmlb_1\n",
      "gid_2019_09_24_atlmlb_kcamlb_1\n",
      "gid_2019_09_24_balmlb_tormlb_1\n",
      "gid_2019_09_24_bosmlb_texmlb_1\n",
      "gid_2019_09_24_chnmlb_pitmlb_1\n",
      "gid_2019_09_24_clemlb_chamlb_1\n",
      "gid_2019_09_24_colmlb_sfnmlb_1\n",
      "gid_2019_09_24_houmlb_seamlb_1\n",
      "gid_2019_09_24_lanmlb_sdnmlb_1\n",
      "gid_2019_09_24_miamlb_nynmlb_1\n",
      "gid_2019_09_24_milmlb_cinmlb_1\n",
      "gid_2019_09_24_minmlb_detmlb_1\n",
      "gid_2019_09_24_nyamlb_tbamlb_1\n",
      "gid_2019_09_24_oakmlb_anamlb_1\n",
      "gid_2019_09_24_phimlb_wasmlb_1\n",
      "gid_2019_09_24_phimlb_wasmlb_2\n",
      "gid_2019_09_24_slnmlb_arimlb_1\n",
      "gid_2019_10_01_milmlb_wasmlb_1\n",
      "gid_2019_09_26_bosmlb_texmlb_1\n",
      "gid_2019_09_26_chnmlb_pitmlb_1\n",
      "gid_2019_09_26_clemlb_chamlb_1\n",
      "gid_2019_09_26_colmlb_sfnmlb_1\n",
      "gid_2019_09_26_houmlb_anamlb_1\n",
      "gid_2019_09_26_lanmlb_sdnmlb_1\n",
      "gid_2019_09_26_miamlb_nynmlb_1\n",
      "gid_2019_09_26_milmlb_cinmlb_1\n",
      "gid_2019_09_26_minmlb_detmlb_1\n",
      "gid_2019_09_26_oakmlb_seamlb_1\n",
      "gid_2019_09_26_phimlb_wasmlb_1\n",
      "gid_2019_09_28_atlmlb_nynmlb_1\n",
      "gid_2019_09_28_balmlb_bosmlb_1\n",
      "gid_2019_09_28_chnmlb_slnmlb_1\n",
      "gid_2019_09_28_cinmlb_pitmlb_1\n",
      "gid_2019_09_28_clemlb_wasmlb_1\n",
      "gid_2019_09_28_detmlb_chamlb_1\n",
      "gid_2019_09_28_detmlb_chamlb_2\n",
      "gid_2019_09_28_houmlb_anamlb_1\n",
      "gid_2019_09_28_lanmlb_sfnmlb_1\n",
      "gid_2019_09_28_miamlb_phimlb_1\n",
      "gid_2019_09_28_milmlb_colmlb_1\n",
      "gid_2019_09_28_minmlb_kcamlb_1\n",
      "gid_2019_09_28_nyamlb_texmlb_1\n",
      "gid_2019_09_28_oakmlb_seamlb_1\n",
      "gid_2019_09_28_sdnmlb_arimlb_1\n",
      "gid_2019_09_28_tbamlb_tormlb_1\n",
      "gid_2019_10_07_atlmlb_slnmlb_1\n",
      "gid_2019_10_07_houmlb_tbamlb_1\n",
      "gid_2019_10_07_lanmlb_wasmlb_1\n",
      "gid_2019_10_07_nyamlb_minmlb_1\n",
      "gid_2019_09_27_atlmlb_nynmlb_1\n",
      "gid_2019_09_27_balmlb_bosmlb_1\n",
      "gid_2019_09_27_chnmlb_slnmlb_1\n",
      "gid_2019_09_27_cinmlb_pitmlb_1\n",
      "gid_2019_09_27_clemlb_wasmlb_1\n",
      "gid_2019_09_27_detmlb_chamlb_2\n",
      "gid_2019_09_27_houmlb_anamlb_1\n",
      "gid_2019_09_27_lanmlb_sfnmlb_1\n",
      "gid_2019_09_27_miamlb_phimlb_1\n",
      "gid_2019_09_27_milmlb_colmlb_1\n",
      "gid_2019_09_27_minmlb_kcamlb_1\n",
      "gid_2019_09_27_nyamlb_texmlb_1\n",
      "gid_2019_09_27_oakmlb_seamlb_1\n",
      "gid_2019_09_27_sdnmlb_arimlb_1\n",
      "gid_2019_09_27_tbamlb_tormlb_1\n",
      "gid_2019_10_05_minmlb_nyamlb_1\n",
      "gid_2019_10_05_tbamlb_houmlb_1\n",
      "gid_2019_10_02_tbamlb_oakmlb_1\n",
      "gid_2019_10_08_houmlb_tbamlb_1\n",
      "gid_2019_09_22_anamlb_houmlb_1\n",
      "gid_2019_09_22_arimlb_sdnmlb_1\n",
      "gid_2019_09_22_bosmlb_tbamlb_1\n",
      "gid_2019_09_22_chamlb_detmlb_1\n",
      "gid_2019_09_22_colmlb_lanmlb_1\n",
      "gid_2019_09_22_kcamlb_minmlb_1\n",
      "gid_2019_09_22_nynmlb_cinmlb_1\n",
      "gid_2019_09_22_phimlb_clemlb_1\n",
      "gid_2019_09_22_pitmlb_milmlb_1\n",
      "gid_2019_09_22_seamlb_balmlb_1\n",
      "gid_2019_09_22_sfnmlb_atlmlb_1\n",
      "gid_2019_09_22_slnmlb_chnmlb_1\n",
      "gid_2019_09_22_texmlb_oakmlb_1\n",
      "gid_2019_09_22_tormlb_nyamlb_1\n",
      "gid_2019_09_22_wasmlb_miamlb_1\n",
      "gid_2019_10_09_slnmlb_atlmlb_1\n",
      "gid_2019_10_09_wasmlb_lanmlb_1\n",
      "gid_2019_10_04_minmlb_nyamlb_1\n",
      "gid_2019_10_04_slnmlb_atlmlb_1\n",
      "gid_2019_10_04_tbamlb_houmlb_1\n",
      "gid_2019_10_04_wasmlb_lanmlb_1\n",
      "gid_2019_10_10_tbamlb_houmlb_1\n",
      "gid_2019_09_25_atlmlb_kcamlb_1\n",
      "gid_2019_09_25_balmlb_tormlb_1\n",
      "gid_2019_09_25_bosmlb_texmlb_1\n",
      "gid_2019_09_25_chnmlb_pitmlb_1\n",
      "gid_2019_09_25_clemlb_chamlb_1\n",
      "gid_2019_09_25_colmlb_sfnmlb_1\n",
      "gid_2019_09_25_houmlb_seamlb_1\n",
      "gid_2019_09_25_lanmlb_sdnmlb_1\n",
      "gid_2019_09_25_miamlb_nynmlb_1\n",
      "gid_2019_09_25_milmlb_cinmlb_1\n",
      "gid_2019_09_25_minmlb_detmlb_1\n",
      "gid_2019_09_25_nyamlb_tbamlb_1\n",
      "gid_2019_09_25_oakmlb_anamlb_1\n",
      "gid_2019_09_25_phimlb_wasmlb_1\n",
      "gid_2019_09_25_slnmlb_arimlb_1\n",
      "gid_2019_10_11_wasmlb_slnmlb_1\n",
      "gid_2019_10_06_atlmlb_slnmlb_1\n",
      "gid_2019_10_06_lanmlb_wasmlb_1\n",
      "gid_2019_10_12_nyamlb_houmlb_1\n",
      "gid_2019_10_12_wasmlb_slnmlb_1\n",
      "gid_2019_10_13_nyamlb_houmlb_1\n",
      "gid_2019_10_14_slnmlb_wasmlb_1\n",
      "gid_2019_10_15_houmlb_nyamlb_1\n",
      "gid_2019_10_15_slnmlb_wasmlb_1\n",
      "gid_2019_10_19_nyamlb_houmlb_1\n",
      "gid_2019_10_18_houmlb_nyamlb_1\n",
      "gid_2019_10_17_houmlb_nyamlb_1\n",
      "gid_2019_09_29_atlmlb_nynmlb_1\n",
      "gid_2019_09_29_balmlb_bosmlb_1\n",
      "gid_2019_09_29_chnmlb_slnmlb_1\n",
      "gid_2019_09_29_cinmlb_pitmlb_1\n",
      "gid_2019_09_29_clemlb_wasmlb_1\n",
      "gid_2019_09_29_detmlb_chamlb_1\n",
      "gid_2019_09_29_houmlb_anamlb_1\n",
      "gid_2019_09_29_lanmlb_sfnmlb_1\n",
      "gid_2019_09_29_miamlb_phimlb_1\n",
      "gid_2019_09_29_milmlb_colmlb_1\n",
      "gid_2019_09_29_minmlb_kcamlb_1\n",
      "gid_2019_09_29_nyamlb_texmlb_1\n",
      "gid_2019_09_29_oakmlb_seamlb_1\n",
      "gid_2019_09_29_sdnmlb_arimlb_1\n",
      "gid_2019_09_29_tbamlb_tormlb_1\n"
     ]
    }
   ],
   "source": [
    "for i in range(q.qsize()):\n",
    "    print(q.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrosheet.org\n",
    "\n",
    "According to the Society for American Baseball Research (SABR), this website contains data that was compiled by ”a small army of volunteers, combing historical sources to try to re-create the play-by-play of every game in baseball history and digitizing it for download and analysis.” While there are a lot of compiled statistics on the site, we’re primarily interested in the play-by-play event data (as we were with the MLB.com data).\n",
    "\n",
    "If you navigate to the page https://www.retrosheet.org/game.htm, you will find the following.\n",
    "\n",
    "1. A link to a listing of players, coaches, managers, and umpires (past and present). This data will be needed for decoding player IDs in the data.\n",
    "2. Links for regular season event files. For each year, there is a link to a zipped file containing three types of data.\n",
    "    1. Event data for each MLB team for that year (.eva and .evn extensions)\n",
    "    2. Roster data for each MLB team for that year (.ros extension)\n",
    "    3. A file named TeamXXXX (where XXXX is the year) that contains a list of MLB teams in year XXXX. This data will be needed for decoding team IDs in the data (among other things).\n",
    "3. Links for post-season event file. Again, for each year there is a link to a zipped file containing the same three types of data. You will need to keep the event files, but the team and roster data will have already been acquired from the regular season files.\n",
    "\n",
    "Your script should do the following.\n",
    "\n",
    "1. Create a directory called Retrosheets.\n",
    "2. Inside Retrosheets,\n",
    "    1. download the list of players, coaches, managers, and umpires to a file called player.csv and\n",
    "    2. create three directories called Events, Teams, and Rosters.\n",
    "3. For each year between 1900 and 2019, download and unzip the regular season event data and distribute the event, roster, and team files to the appropriate directories.\n",
    "4. For each year between 1900 and 2019, download and unzip the post-season event data and copy the event files to the appropriate directory. You do not need to keep the Team and Roster data since that will already have been acquired from the regular season download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: /home/david/Documents/2019Fall/Data3402/Projects/Project_1/Retrosheets\n",
      "Now the current working directory is: /home/david/Documents/2019Fall/Data3402/Projects/Project_1\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "if not (os.path.exists('./Retrosheets/')):\n",
    "    os.mkdir('./Retrosheets/')\n",
    "\n",
    "os.chdir('./Retrosheets')\n",
    "wget.download('https://www.retrosheet.org/retroID.htm', 'player.csv')\n",
    "\n",
    "if not (os.path.exists('./Events/')):\n",
    "    os.mkdir('./Events/')\n",
    "if not (os.path.exists('./Teams/')):\n",
    "    os.mkdir('./Teams/')\n",
    "if not (os.path.exists('./Rosters/')):\n",
    "    os.mkdir('./Rosters/')\n",
    "\n",
    "for year in range(2017, 2020):\n",
    "    \n",
    "    try:\n",
    "        wget.download('https://www.retrosheet.org/events/'+str(year)+'eve.zip')\n",
    "        \n",
    "        zf = ZipFile(str(year)+'eve.zip', 'r')\n",
    "        zf.extractall()\n",
    "        zf.close()\n",
    "\n",
    "        r_event = '\\d+\\w+.EVN'\n",
    "        r_rost = '\\d+\\w+.ROS'\n",
    "        r_team = 'TEAM\\d+'\n",
    "        r_eva = '\\d+\\w+.EVA'\n",
    "        r_zip = '\\d+\\w+.zip'\n",
    "\n",
    "\n",
    "        for file in os.listdir():\n",
    "            if(re.search(r_event, file)):\n",
    "                os.rename(file, './Events/'+str(file))\n",
    "            if(re.search(r_rost, file)):\n",
    "                os.rename(file, './Rosters/'+str(file))\n",
    "            if(re.search(r_team, file)):\n",
    "                os.rename(file, './Teams/'+str(file))\n",
    "            elif(re.search(r_eva, file) or re.search(r_zip, file)):\n",
    "                os.remove(file)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print('The current working directory was: {}'.format(os.getcwd()))\n",
    "os.chdir('/home/david/Documents/2019Fall/Data3402/Projects/Project_1')\n",
    "print('Now the current working directory is: {}'.format(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Lahman Database\n",
    "The Lahman Database contains a large number of statistics for every MLB player aggregated by year. You won’t be able to determine if Jose Altuve got a hit in the 6th inning of the Astros’ 2018 season opener, but you will be able to find out his batting average, RBI count, etc. for the 2018 season.\n",
    "\n",
    "Your script should do the following:\n",
    "\n",
    "1. Create a directory called Lahman.\n",
    "2. Download the 2017 Lahman database in CSV format from http://www.seanlahman.com/baseball-archive/statistics\n",
    "3. Unzip the archive into Lahman\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the file...\n",
      "\n",
      "Extracting the file to Lahman...\n",
      "\n",
      "Removing unecessary zip file...\n",
      "\n",
      "The current working directory is: /home/david/Documents/2019Fall/Data3402/Projects/Project_1/Lahman\n",
      "Now the current working directory is: /home/david/Documents/2019Fall/Data3402/Projects/Project_1\n"
     ]
    }
   ],
   "source": [
    "if not (os.path.exists('./Lahman/')):\n",
    "    os.mkdir('./Lahman/')\n",
    "\n",
    "os.chdir('./Lahman/')\n",
    "\n",
    "print('Downloading the file...\\n')\n",
    "wget.download('http://seanlahman.com/files/database/baseballdatabank-master_2018-03-28.zip')\n",
    "\n",
    "print('Extracting the file to Lahman...\\n')\n",
    "file = ZipFile('baseballdatabank-master_2018-03-28.zip', 'r')\n",
    "file.extractall()\n",
    "file.close()\n",
    "\n",
    "print('Removing unecessary zip file...\\n')\n",
    "os.remove('baseballdatabank-master_2018-03-28.zip')\n",
    "\n",
    "print('The current working directory was: {}'.format(os.getcwd()))\n",
    "os.chdir('/home/david/Documents/2019Fall/Data3402/Projects/Project_1')\n",
    "print('Now the current working directory is: {}'.format(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Miscellanea\n",
    "\n",
    "And that’s the project. Your script should be able to be run as a cron job once daily to pick up any newly released data. It should not just re-download all of the data every time it runs. It should check each date, determine if any data is missing, and only download what is needed. Upload your completed scripts to blackboard by 11:59pm Thursday, 10/10/19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: /home/david/Documents/2019Fall/Data3402/Projects/Project_1/Lahman\n",
      "Now the current working directory is: /home/david/Documents/2019Fall/Data3402/Projects/Project_1\n"
     ]
    }
   ],
   "source": [
    "print('The current working directory was: {}'.format(os.getcwd()))\n",
    "os.chdir('/home/david/Documents/2019Fall/Data3402/Projects/Project_1')\n",
    "print('Now the current working directory is: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
